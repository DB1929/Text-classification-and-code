3.1	文本数据的处理
3.1.1	数据的搜集
使用python编写爬虫代码，从美团上爬取了烟台约40页的火锅店，每页有15家店。为了获得对火锅餐饮的较为客观的评论，从每个店面爬取10条相关评论。对一些店铺评论进行简单的筛选，去除得到空的字符串，最后获得1404条评论。
3.1.2	数据清洗
考虑到评论的过程中可能会存在的一些复制粘贴的行为，爬取的数据中可能会有一些重复的评论，因此首先对这1405条数据进行简单的去重。由于一些表情符号和其他符号会因为编码的问题导致程序无法识别文档，所以还去除了这类符号。最后去了所有常见的词语，因为这些词语在文本分类中所出现的频率过高，对分类结果的准确性有着非常不好的影响。处理完之后还剩余1225条评论。
3.1.3	数据标记
将数据导入到excel中，手动将数据标记为正类和负类，接着将数据输入到程序中与不同的文本对应起来。为后续文本数据向量化作准备。
3.1.4	去除停用词
先使用jieba分词将清洗过一遍的数据进行分词，接着再开始去除文本中的停用词。有的文本中还存在有大量的类似于“了”、“的”、“况且”等语气词与连词，这些词汇在文本中都是一些比较常见的单词，不能够很好地代表一段文本的特性，因此将其去除。之前从网络上搜集到的停用词还包含有“一般”、“不”、“好”等停用词，结合从美团上爬取到的评论，这三者在文本分类中起着重要的作用，它们能够很好地代表一段文本的情感态度。例如在初始的文本为：一般般吧，无功无过。这类包含有一般词汇的句子就代表评论者对这个店家的不太满意，在手工文本分类的过程中我们将其标记为消极的一个文本；还有一类文本：不好，不太建议去，如果使用停用词“不”的话，就变成了：好，建议去，这很明显与它所表达的情感完全相反，如果不去掉的话会严重影响分类的精度。对于分词过后的评论，应该首先满足的一点是我们人能够快速而较为准确地辨别出这是哪一类。还有一个字“好”，如果评论是“好吃”，去掉好的话，就变成了“吃”，这可以看作是中性词汇，无法判断出其中的正负面情感。这样下来，处理完之后还剩余1220条评论。
3.1.5	将文本数据转化为数值向量
一般所使用的方法是统计每个词汇在文本中所出现的频率，用这些数字将单词转化为能够代表文章语义的数字，也就是特征提取，特征关键字提取的主要方法有TextRank、Tf-IDF、topic model、LDA。文本数据向量化的方法主要有one-hot矩阵、Word2Vec。本文主要选取IF-IDF和TextRank来进行文本特征的选择，再利用python中的sklearn模块的CountVectorizer、TfidfTransformer中的max_feature以及Word2Vec来实现维度控制。之后再进行进一步的降维，传入到分类器中进行训练、分类、测试。
3.1.6	数据降维
在本文中的主要使用两种类型的方法来实现向量降维。一种是通过直接的在向量数值表示之后利用CounterVectorizer和TfidfVectorizer中的max_features参数来控制输入变量的维数；另一种则是使用数学上的降维方法来降低数据的输入维度。其中主流的降维方法有主成分分析法PCA、线性判别法LDA、因子分析、局部线性潜入法LLE和Laplacian Eigenmaps拉普拉斯线性映射来实现对初始的数据矩阵进行降维。在这里我们选取两种降维方法，PCA和LDA。
